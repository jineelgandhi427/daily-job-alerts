
import os
import requests
from datetime import datetime
from bs4 import BeautifulSoup

# Keywords based on resume
KEYWORDS = [
    "mechatronics", "simulation", "sensor", "test engineer", "digital twin",
    "solidworks", "fusion 360", "robotics", "python", "ros", "unity", "test bench"
]

SEARCH_TERMS = [
    "mechatronics engineer", "sensor test engineer", "simulation engineer",
    "robotics developer", "digital twin engineer", "test development"
]

SEARCH_URLS = {
    "Indeed": "https://de.indeed.com/jobs?q={query}&l=Germany&fromage=1",
    "StepStone": "https://www.stepstone.de/en/jobs/{query}/germany/",
    "Jobtensor": "https://jobtensor.com/Jobs?q={query}&l=germany"
}

LINKEDIN_URLS = {
    "LinkedIn - Mechatronics (24h)": "https://www.linkedin.com/jobs/search/?keywords=Mechatronics%20Engineer&location=Germany&f_TP=1&sortBy=DD",
    "LinkedIn - Simulation (24h)": "https://www.linkedin.com/jobs/search/?keywords=Simulation%20Engineer&location=Germany&f_TP=1&sortBy=DD"
}

def fetch_links(query, url_template, site_name):
    jobs = []
    headers = {"User-Agent": "Mozilla/5.0"}
    url = url_template.format(query=query.replace(" ", "+"))
    try:
        response = requests.get(url, headers=headers, timeout=10)
        soup = BeautifulSoup(response.text, "html.parser")
        for link in soup.find_all("a", href=True):
            title = link.get_text(strip=True).lower()
            href = link["href"]
            if any(k in title for k in KEYWORDS) and "ausbildung" not in title:
                full_link = href if href.startswith("http") else f"https://{site_name.lower()}.de{href}"
                jobs.append((title.title(), site_name, full_link))
    except Exception as e:
        print(f"‚ùå Error fetching from {site_name}: {e}")
    print(f"‚úÖ {site_name}: {len(jobs)} jobs found.")
    return jobs

def collect_jobs():
    results = []
    for term in SEARCH_TERMS:
        for site, url in SEARCH_URLS.items():
            results += fetch_links(term, url, site)
    for label, link in LINKEDIN_URLS.items():
        results.append((label, "LinkedIn", link))
    return results

def format_html(jobs):
    date = datetime.now().strftime('%d %B %Y')
    if not jobs:
        return "<p><b>No matching jobs found today.</b></p>"
    html = f"<h3>üîç Matched Jobs ‚Äì {date}</h3><ul>"
    for title, site, link in jobs:
        html += f"<li><b>{title}</b> ‚Äì <i>{site}</i><br><a href='{link}'>Apply Now</a></li><br>"
    html += "</ul><br><i>This is an automated message.</i>"
    return html

def write_txt(jobs, path):
    with open(path, "w", encoding="utf-8") as f:
        for title, site, link in jobs:
            f.write(f"{title} ‚Äì {site}\n{link}\n\n")

# Collect job matches
jobs = collect_jobs()
html_body = format_html(jobs)
write_txt(jobs, "matched_jobs.txt")

# Email via Brevo API
API_KEY = os.getenv("BREVO_API_KEY")
headers = {
    "accept": "application/json",
    "api-key": API_KEY,
    "content-type": "application/json"
}
payload = {
    "sender": {"name": "Jineel Gandhi", "email": "jineelgandhi426@gmail.com"},
    "to": [{"email": "jineelgandhi426@gmail.com"}],
    "subject": "üîî Daily Germany Job Alerts ‚Äì Profile Matched",
    "htmlContent": html_body
}

# Attach plain text file as fallback
try:
    with open("matched_jobs.txt", "r", encoding="utf-8") as f:
        file_content = f.read()
    payload["attachment"] = [{
        "content": file_content.encode("utf-8").decode("utf-8"),
        "name": "matched_jobs.txt"
    }]
except Exception as e:
    print("‚ö†Ô∏è Could not attach fallback job list:", str(e))

try:
    response = requests.post("https://api.brevo.com/v3/smtp/email", json=payload, headers=headers)
    if response.status_code == 201:
        print("‚úÖ Email sent successfully via Brevo API.")
    else:
        print("‚ùå Failed to send email:", response.status_code, response.text)
except Exception as e:
    print("‚ùå Exception occurred:", str(e))
